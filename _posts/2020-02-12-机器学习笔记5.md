---
layout: post
---

### 1. 人脸识别技术原理
人脸识别，是基于人的脸部特征信息进行身份识别的一种生物识别技术。用摄像机或摄像头采集含有人脸的图像或视频流，并自动在图像中检测和跟踪人脸，进而对检测到的人脸进行脸部的一系列相关技术，通常也叫做人像识别、面部识别。

**人脸识别过程：**

人脸识别主要分为人脸检测（face detecTIon）、特征提取（feature extracTIon）和人脸识别（face recogniTIon）三个过程。
 
+ 人脸检测：

人脸检测是指从输入图像中检测并提取人脸图像，通常采用 haar 特征和 Adaboost 算法 训练级联分类器对图像中的每一块进行分类。如果某一矩形区域通过了级联分类器，则被判别为人脸图像。
 
+ 特征提取：

特征提取是指通过一些数字来表征人脸信息，这些数字就是我们要提取的特征。
 
常见的人脸特征分为两类，一类是**几何特征**，另一类是**表征特征**。

**几何特征**是指眼睛、鼻子和嘴等面部特征之间的几何关系，如距离、面积和角度等。由于算法利用了一些直观的特征，计算量小。 
不过，由于其所需的特征点不能精确选择，限制了它的应用范围。另外，当光照变化、人脸有外物遮挡、面部表情变化时，特征变化较大。所以说，这类算法只适合于人脸图像的粗略识别，无法在实际中应用。
 
**表征特征**利用人脸图像的灰度信息，通过一些算法提取全局或局部特征。其中比较常用的特征提取算法是 LBP 算法。
**LBP 方法**首先将 图像分成若干区域，在每个区域的像素 640x960 邻域中用中心值作阈值化，将结果看成是二进制数。 LBP 算子的特点是对单调 灰度变化保持不变。每个区域通过这样的运算得到一组直方图，然后将所有的直方图连起来组成一个大的直方图并进行直方图匹配计算进行分类。

+ 人脸识别：

这里提到的人脸识别是狭义的人脸识别，即将待识别人脸所提取的特征与数据库中人脸的特征进行对比，根据相似度判别分类。而人脸识别又可以分为两个大类：一类是确认，这是人脸图像与数据库中已存的该人图像比对的过程，回答你是不是你的问题;
 
另一类是辨认，这是人脸图像与数据库中已存的所有图像匹 配的过程，回答你是谁的问题。显然，人脸辨认要比人脸确认困难，因为辨认需要进行海量数据的匹配。常用的分类器有最近邻分类器、支持向量机等。

### 2. numpy多维数组转一维

函数可以用reshape(), resize(), flatten(),ravel()

	>>> import numpy as np
	>>> p = np.array([[1,2,3],[4,5,6],[7,8,9]])
	>>> p.reshape((1))
	Traceback (most recent call last):
	  File "<stdin>", line 1, in <module>
	ValueError: cannot reshape array of size 9 into shape (1,)
	>>> p.reshape((-1))
	array([1, 2, 3, 4, 5, 6, 7, 8, 9])
	>>> p
	array([[1, 2, 3],
	       [4, 5, 6],
	       [7, 8, 9]])
	>>> p.flatten()
	array([1, 2, 3, 4, 5, 6, 7, 8, 9])
	>>> p
	array([[1, 2, 3],
	       [4, 5, 6],
	       [7, 8, 9]])
	>>> p.reshape((1,-1))
	array([[1, 2, 3, 4, 5, 6, 7, 8, 9]])
	>>>

+　ndarray.flatten(order='C')

order : {‘C’, ‘F’, ‘A’, ‘K’}, 
 
order选项：

‘C’ 按行展开，默认值.

‘F’ 按列展开. 

‘A’ 如果内存中的存储与Fortran类似,则按列展开,否则按行展开 
otherwise. 

‘K’ 按内存中元素的顺序展开. 


+　多维数组降为一维数组numpy.ravel()

语法：np.ravel(A，’T’) 

参数：1）A是多维数组 2）’T‘代表按行优先，为默认值；’F‘代表按列优先
 
返回值：一维数组

	>>> p.ravel()
	array([1, 2, 3, 4, 5, 6, 7, 8, 9])
	>>> p.ravel('T')
	Traceback (most recent call last):
	  File "<stdin>", line 1, in <module>
	TypeError: order not understood
	>>> p.ravel('F')
	array([1, 4, 7, 2, 5, 8, 3, 6, 9])

#### 2.1 numpy.squeeze 函数

squeeze 函数：从数组的形状中删除单维度条目，即把shape中为1的维度去掉

用法：numpy.squeeze(a,axis = None)
	
	 1）a表示输入的数组；
	 2）axis用于指定需要删除的维度，但是指定的维度必须为单维度，否则将会报错；
	 3）axis的取值可为None 或 int 或 tuple of ints, 可选。若axis为空，则删除所有单维度的条目；
	 4）返回值：数组
	 5) 不会修改原数组；

	>>> a = e.reshape(1,1,10)
	>>> a
	array([[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]])
	>>> np.squeeze(a)
	array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])

#### 2.2 argsort()函数

numpy.argsort(a, axis=-1, kind=’quicksort’, order=None)

功能: 将矩阵a按照axis排序，并返回排序后的下标,argsort()是将元素从小到大排序后，提取对应的索引index，然后输出到y

参数: a:输入矩阵， axis:需要排序的维度

返回值: 输出排序后的下标

	>>> p
	array([[1, 2, 3],
	       [4, 5, 6],
	       [7, 8, 9]])
	>>> p.argsort()
	array([[0, 1, 2],
	       [0, 1, 2],
	       [0, 1, 2]], dtype=int64)


### 3.贝叶斯定理

条件概率:

P(B|A) = P(AB) / P(A)

-->乘法定理:P(AB) = P(B|A) P(A)

全概率:

P(A) = 	∑　P(A|Bi)P(Bi)

贝叶斯公式:

P(Bi|A) = P(ABi) / P(A)

-->P(Bi|A) = P(A|Bi)P(Bi) / ∑　P(A|Bi)P(Bi)


![](https://bkimg.cdn.bcebos.com/pic/1ad5ad6eddc451da062e7cb7bafd5266d1163295)

[（转）贝叶斯的理解](https://www.jianshu.com/p/4d5e3655269e)

### 4.本体知识推理

OWL本体语言:

W3C Web本体语言（OWL）是一种语义Web语言，旨在表示有关事物，事物组以及事物之间关系的丰富而复杂的知识。OWL是一种基于计算逻辑的语言，因此OWL中表达的知识可以被计算机程序利用，例如，以验证该知识的一致性或使隐式知识变得明确。OWL文档（称为本体）可以在万维网上发布，并且可以引用其他OWL本体或从中引用。OWL是W3C语义Web技术堆栈的一部分，其中包括RDF，RDFS，SPARQL等。
 

### 5. XGBoost

Xgboost简介

　　Xgboost是Boosting算法的其中一种，Boosting算法的思想是将许多弱分类器集成在一起，形成一个强分类器。因为Xgboost是一种提升树模型，所以它是将许多树模型集成在一起，形成一个很强的分类器。而所用到的树模型则是CART回归树模型。

### 6.pandas 取列名

df.ix[]既可以通过整数索引进行数据选取，也可以通过标签索引进行数据选取，换句话说，df.ix[]是df.loc[]和df.iloc[]的功能集合，且在同义词选取中，可以同时使用整数索引和标签索引

	1.[column for column in df] 
	2.df.columns.values 返回 array 
	3.list(df) 
	4.df.columns 返回Index，可以通过 tolist(), 或者 list（array） 转换为list

+ 缺失数据填充fillna（）

print(data.fillna(data.mean()))  ### 用每列特征的均值填充缺失数据

print(data.fillna(data.median())) ### 用每列特征的中位数填充缺失数据

print(data.fillna(method='bfill'))   ### 用相邻后面（back）特征填充前面空值

+ pandas的qcut()方法

pandas的qcut可以把一组数字按大小区间进行分区

	data = pd.Series([0,8,1,5,3,7,2,6,10,4,9])
	print(pd.qcut(data,[0,0.5,1],labels=['small number','large number']))
	
	 qcut() 方法第一个参数是数据,第二个参数定义区间的分割方法,
	比如这里把数字分成两半,那就是 [0, 0.5, 1] 如果要分成4份,
	就是 [0, 0.25, 0.5, 0.75, 1] ,也可以不是均分,比如
	 [0, 0.1, 0.2, 0.3, 1] ,这就就会按照 1:1:1:7 进行分布

	pd.qcut(range(5), 4)
	Out[3]: 
	[(-0.001, 1.0], (-0.001, 1.0], (1.0, 2.0], (2.0, 3.0], (3.0, 4.0]]
	Categories (4, interval[float64]): 
	[(-0.001, 1.0] < (1.0, 2.0] < (2.0, 3.0] < (3.0, 4.0]]

	pd.qcut(range(5), 3, labels=["good", "medium", "bad"])
	Out[5]: 
	[good, good, medium, bad, bad]
	Categories (3, object): [good < medium < bad]

+ pandas.Series.str.split()

Series.str.split（self，pat = None，n = -1，expand = False ）

	expand-布尔值，默认为False
	将拆分的字符串展开为单独的列。	
	如果为True，则返回DataFrame / MultiIndex扩展维。	
	如果为False，则返回包含字符串列表的Series / Index。

+ pandas中pd.groupby()

类似SQL中的分组



### 7.seaborn
Seaborn是一种基于matplotlib的图形可视化python libraty。它提供了一种高度交互式界面，便于用户能够做出各种有吸引力的统计图表。

Seaborn其实是在matplotlib的基础上进行了更高级的API封装，从而使得作图更加容易，在大多数情况下使用seaborn就能做出很具有吸引力的图，而使用matplotlib就能制作具有更多特色的图。应该把Seaborn视为matplotlib的补充，而不是替代物。同时它能高度兼容numpy与pandas数据结构以及scipy与statsmodels等统计模式。掌握seaborn能很大程度帮助我们更高效的观察数据与图表，并且更加深入了解它们。

**设置多子图与图像大小**

    # 分类图 （Categorical Plots）
    num= len(list(data))
    fig, axes = plt.subplots(nrows=num-1, ncols=1,figsize=(12, 6.5))
    colors = [plt.cm.Spectral(i / float(num-1))  for i in range(num)]
    for i,ax in enumerate(axes.flatten()):
        f = sns.countplot(x=data.iloc[:,i], hue='Survived', data=data,
                          ax=ax)
    plt.legend()
    plt.show()

Python提供了许多魔法命令，使得在IPython环境中的操作更加得心应手。魔法命令都以%或者%%开头，以%开头的成为行命令，%%开头的称为单元命令。行命令只对命令所在的行有效，而单元命令则必须出现在单元的第一行，对整个单元的代码进行处理。

**%matplotlib inline** 可以在Ipython编译器里直接使用，功能是可以内嵌绘图，并且可以省略掉plt.show()这一步。

### 8.sklearn.model_selection.ShuffleSplit

sklearn.model_selection.ShuffleSplit类用于将样本集合随机“打散”后划分为训练集、验证集：

class sklearn.model_selection.ShuffleSplit(n_splits=10, test_size=’default’, train_size=None, random_state=None)

	参数：
	
	n_splits:int, 划分训练集、测试集的次数，默认为10
	test_size:float, int, None, default=0.1； 
	测试集比例或样本数量，该值为[0.0, 1.0]内的浮点数时，表示测试集占总样本的比例；
	random_state:int, RandomState instance or None；
	随机种子值，默认为None

	ShuffleSplit类方法包括get_n_splits、split，前者用于返回划分训练集、测试集的次数：

	split(X, y=None, groups=None)
	参数：
	X：array-like, shape (n_samples, n_features)；样本特征集合
	y：array-like, shape (n_samples,)；
	样本标记集合，该值设置时需与X的样本数量(n_samples)一致
	groups：该参数在此处不生效
	返回值：包含训练集、测试集索引值的迭代器

+ sklearn.model_selection.GroupShuffleSplit

作用与ShuffleSplit相同，不同之处在于GroupShuffleSplit先将待划分的样本集分组，再按照分组划分训练集、测试集。

+ DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.
  from numpy.core.umath_tests import inner1d

	The solution to remove this error is as follow:
	
	1. As I can see you have Scikit-Learn version 0.19.2 installed you need to get the latest version. To do so enter the following command
	pip3 install --force-reinstall scikit-learn==0.20rc1
	This will install the latest versions of scikit-learn, scipy and numpy. Your deprecation warnings will now not exist.
	
	2. Although you'll next get a new warning. This time regarding the file in the scikit-learn library called cloudpickle \sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py.
	To overcome this warning you got to edit the code which python shows us.
	Just do a sudo idle3 onto the file and edit the lines which says :
	import imp
	from imp import find_module
	to
	import importlib
	3. Next go to the funtion find_module and change the line
	file, path, description = find_module(path)
	to
	file, path, description = importlib.utils.find_spec(path).
	This must solve the deprecation warnings in the scikit-learn libraries.

[https://stackoverflow.com/questions/51986414/deprecationwarning-numpy-core-umath-tests](https://stackoverflow.com/questions/51986414/deprecationwarning-numpy-core-umath-tests)

+ 特征选择 (feature_selection)

[特征选择 (feature_selection)](https://www.cnblogs.com/stevenlk/p/6543628.html)

递归特征消除 (Recursive Feature Elimination)
　　递归消除特征法使用一个基模型来进行多轮训练，每轮训练后，移除若干权值系数的特征，再基于新的特征集进行下一轮训练。

RFECV 通过交叉验证的方式执行RFE，以此来选择最佳数量的特征：对于一个数量为d的feature的集合，他的所有的子集的个数是2的d次方减1(包含空集)。指定一个外部的学习算法，比如SVM之类的。通过该算法计算所有子集的validation error。选择error最小的那个子集作为所挑选的特征。

	dtree_rfe = feature_selection.RFECV(dtree, step=1, scoring='accuracy', cv=cv_split)
	dtree_rfe.fit(X, y)

+ 机器学习：集成学习（Soft Voting Classifier）

Hard Voting Classifier：根据少数服从多数来定最终结果；

Soft Voting Classifier：将所有模型预测样本为某一类别的概率的平均值作为标准，概率最高的对应的类型为最终的预测结果；

	from sklearn.linear_model import LogisticRegression
	from sklearn.svm import SVC
	from sklearn.tree import DecisionTreeClassifier
	from sklearn.ensemble import VotingClassifier
	
	# 实例化
	voting_clf = VotingClassifier(estimators=[
	    ('log_clf', LogisticRegression()),
	    ('svm_clf', SVC()),
	    ('dt_clf', DecisionTreeClassifier(random_state=666))
	], voting='hard')
	
	voting_clf.fit(X_train, y_train)
	voting_clf.score(X_test, y_test)

+ xgboost, ValueError: feature_names may not contain [, ] or <

原因:输入数据的列名中含有[,],<

解决办法: 去掉
	
	import re
	
	regex = re.compile(r"\[|\]|<", re.IGNORECASE)
	
	X_train.columns = [regex.sub("_", col) if any(x in str(col) 
		for x in set(('[', ']', '<'))) else col for col in X_train.columns.values]

参考：

1. [my coding.net](http://zhwa3232.coding.me/baibingqianlan.github.io/)
2. [人脸识别技术原理分析以及几种典型的解决方案对比](https://www.eefocus.com/communication/400172/r0)
3. [数据挖掘领域十大经典算法之—AdaBoost算法（超详细附代码）](https://blog.csdn.net/fuqiuai/article/details/79482487)
4. [numpy的squeeze函数](https://blog.csdn.net/tracy_leaf/article/details/79297121)
5. [（转）贝叶斯的理解](https://www.jianshu.com/p/4d5e3655269e)
6. [知识推理](https://blog.csdn.net/liangwqi/article/details/82497530)
7. [https://www.w3.org/2001/sw/wiki/OWL](https://www.w3.org/2001/sw/wiki/OWL)
8. [https://www.w3.org/TR/2012/REC-owl2-overview-20121211/#Documentation_Roadmap](https://www.w3.org/TR/2012/REC-owl2-overview-20121211/#Documentation_Roadmap)
9. [终于有人说清楚了–XGBoost算法](https://www.lizenghai.com/archives/21670.html)
10. [Python机器学习笔记：XgBoost算法](Python机器学习笔记：XgBoost算法)
11. [Kaggle比赛心得](https://www.jianshu.com/p/47282e2fc5d7/)
12. [Kaggle入门，看这一篇就够了](https://zhuanlan.zhihu.com/p/25686876)
13. [https://blog.csdn.net/th_num/article/details/80296254](https://blog.csdn.net/th_num/article/details/80296254)
14. [pandas的qcut()方法](https://www.cnblogs.com/liulangmao/p/9342544.html)
15. [pandas之cut(),qcut()](https://www.cnblogs.com/nicetoseeyou/p/10655422.html)
16. [https://www.kaggle.com/ldfreeman3/a-data-science-framework-to-achieve-99-accuracy](https://www.kaggle.com/ldfreeman3/a-data-science-framework-to-achieve-99-accuracy)
17. [https://blog.csdn.net/hurry0808/article/details/80797969](https://blog.csdn.net/hurry0808/article/details/80797969)
18. [python数据分析之pandas数据选取：`df[] df.loc[] df.iloc[] df.ix[] df.at[] df.iat[]`](https://www.cnblogs.com/chenhuabin/p/10485549.html)

